# -*- coding: utf-8 -*-
"""CRCNEW.ipynb

Automatically generated by Colab.

Original file is located at
"""

import numpy as np
import pandas as pd
import seaborn as sb
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Lasso
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import recall_score, make_scorer, confusion_matrix, roc_curve, auc, accuracy_score
from sklearn.utils.multiclass import unique_labels
from statsmodels.stats.proportion import proportion_confint
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
import sklearn.svm as svm
from sklearn.svm import SVC
from google.colab import drive
from sklearn.model_selection import cross_val_score
drive.mount('/content/drive')

def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix. shows how well classifier performs
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    classes = classes[unique_labels(y_true, y_pred)]
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    fig, ax = plt.subplots()
    ax.axis('equal')
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout();
    return ax

def compute_performance(yhat, y):
    # First, get tp, tn, fp, fn
    tp = sum(np.logical_and(yhat == 1, y == 1))
    tn = sum(np.logical_and(yhat == 0, y == 0))
    fp = sum(np.logical_and(yhat == 1, y == 0))
    fn = sum(np.logical_and(yhat == 0, y == 1))

    print(f"tp: {tp} tn: {tn} fp: {fp} fn: {fn}")

    # Accuracy
    trainacc = (tp + tn) / (tp + tn + fp + fn)

    # Precision
    # "Of the ones I labeled +, how many are actually +?"
    #precision = tp / (tp + fp)

    # Recall
    # "Of all the + in the data, how many do I correctly label?"
    recall = tp / (tp + fn)

    # Sensitivity
    # "Of all the + in the data, how many do I correctly label?"
    sensitivity = recall

    # Specificity
    # "Of all the - in the data, how many do I correctly label?"
    specificity = tn / (fp + tn)

    # Print results

    #print("Acc:",round(trainacc,3),"Rec:",round(recall,3),"Prec:",round(precision,3),
    #      "Sens:",round(sensitivity,3),"Spec:",round(specificity,3))
    print("Acc:",round(trainacc,3),"Rec:",round(recall,3),
          "Sens:",round(sensitivity,3),"Spec:",round(specificity,3))
        # Print CI for accuracy
    # We can use statsmodel's proportion_confint that uses a normal approximation
    # to th binomial distribution
    # proportion_confint(count, nobs, alpha=0.05, method='normal')
    print("CI for accuracy: ", proportion_confint(tp + tn, tp + tn + fp + fn ))

#Import Database
url = "/content/drive/MyDrive/TEmporary/CRC .xlsx"
SBRT = pd.read_excel(url)

#Drop Columns with high NA values or are irrelevant to outcome. PREV_SYSTX and ADJUVANT were not selected as significant features, so their
#associated types were left out.
SBRT = SBRT.drop(['RAS_STATUS','PREV_SYS_TX_TYPE','Unnamed: 28','ADJUVANT_TYPE','ID_PATIENT','ID_LESION'],axis=1)
SBRT = SBRT.dropna()
y = SBRT.EARLY_DIS_PROG
dummyy = pd.get_dummies(SBRT['EARLY_DIS_PROG'],drop_first = False,prefix='EARLY_DIS_PROG')

#Using dummy variables for categorical variables.
loc = SBRT['LOCATION']
primloc = SBRT['PRIMARY_LOCATION']
ecog = SBRT['ECOG']

x = SBRT.drop(['EARLY_DIS_PROG','LOCATION','PRIMARY_LOCATION','ECOG'],axis='columns')    #Drop them out of SBRT
dummyloc = pd.get_dummies(loc,drop_first = False,prefix='LOCATION') #Dummy the categorical variables
x = x.join(dummyloc) #put the categorical dummies back in.
dummyprimloc = pd.get_dummies(primloc,drop_first= False,prefix='PRIMARY_LOCATION')
x = x.join(dummyprimloc)
dummyecog = pd.get_dummies(ecog, drop_first=False,prefix='ECOG')
x = x.join(dummyecog)


#Test train split of 0.3/0.7, stratified on y.
xtrain, xtest, ytrain, ytest = train_test_split(x,dummyy,test_size=0.3,random_state=0, stratify=y)
ytrain = ytrain['EARLY_DIS_PROG_1.0']

ytest = ytest['EARLY_DIS_PROG_1.0']
xtrain

#Scaling to ensure equal weights
scaler =StandardScaler()
Xtrain_scale = scaler.fit_transform(xtrain)
Xtrain_scale = pd.DataFrame(Xtrain_scale,columns = x.columns)


Xtest_scale = scaler.fit_transform(xtest)
Xtest_scale = pd.DataFrame(Xtest_scale,columns = x.columns)
Xtrain_scale

#Graphing of the coefficient path for different values, graph not used due to convoluted graph result.

regularization_strength = np.logspace(-2, 3, 50)

coefs = np.zeros((regularization_strength.size, x.shape[1]))
#regularization_strength, how much disproportional emphasis to put on more important features
# 2)
for i,L in enumerate(regularization_strength):
    lasso_pipe = Pipeline([
    ('logistic_regression', LogisticRegression(penalty = 'l2',
                                               solver='liblinear',
                                               max_iter=1000,
                                               C = 1.0/L)) # Note, C = 1/lambda from question 3
    ])

    lasso_pipe.fit(Xtrain_scale, ytrain)
    coefs[i] = lasso_pipe.named_steps['logistic_regression'].coef_

# 3)

fig, ax = plt.subplots(dpi = 120)
ax.plot(np.log(regularization_strength), coefs)

ax.set_xlabel(r'$\log(\lambda)$', fontsize = 16)
ax.set_ylabel(r'$\hat{\beta}$', fontsize = 16)
ax.set_title('Coefficient Path', fontsize = 18)
ax.set_xlim(3,None)
ax.set_ylim(-0.1,0.1)

namelist =[]


for i, name in enumerate(SBRT.columns[:-1]):

    ax.annotate(name, xy = (0, coefs[0,i]), ha = 'left', fontsize = 8)
    namelist.append(name)

ax.legend(namelist,loc='upper center',bbox_to_anchor=(0.5, -0.05))

#Logistic regression. While confusion matrix is not ideal with this C value, the number of features selected were preferable to better C values,
#Additionally, the features produced good results when put into the other models.
#Results display the true positive, true negative, false positive and false negative counts as well as the calculated
#accuracy, specificity, sensitivity and recall.

# The table of coefficients shows how significant each feature was.
Lasso_model = LogisticRegression(C=0.2,penalty='l1',solver='liblinear',max_iter=10000)
Lasso_model_LR = Lasso_model.fit(Xtrain_scale,ytrain)

ypred = Lasso_model_LR.predict(Xtest_scale)



ypred_prob = Lasso_model_LR.predict_proba(Xtest_scale)

compute_performance(ypred_prob[:,1]>0.6,ytest)
plot_confusion_matrix( ytest,ypred_prob[:,1]>0.6,classes = unique_labels([0,1]))

np.transpose(Lasso_model_LR.coef_)
best_coef = pd.DataFrame(np.transpose(Lasso_model_LR.coef_),index=Xtrain_scale.columns,columns = ["COEFFICIENT"])
best_coef

#AUC and ROC curve for the logistic regression model.
fprl,tprl,thresholds = roc_curve(ytest,ypred_prob[:,1])

AUC_value = auc(fprl,tprl)

ax = plt.plot(fprl,tprl,label=AUC_value)
plt.title('ROC Curve for LR Classification')
plt.legend()
plt.xlabel('FPR')
plt.ylabel('TPR')

#Lasso selection using the above logistic regression models.
Lasso_model = SelectFromModel(LogisticRegression(C=0.2,penalty='l1',solver='liblinear',max_iter=10000))
Lasso_model_LR = Lasso_model.fit(Xtrain_scale,ytrain)


lr_support = Lasso_model_LR.get_support()
saved_feats=x.columns[(lr_support).ravel().tolist()]
print(str(len(saved_feats)),'selected features')
print(saved_feats)

Xtrain_LASSO= Xtrain_scale[saved_feats]
Xtest_LASSO = Xtest_scale[saved_feats]
Xtest_LASSO
lr_support
Lasso_model_LR.get_support

def plotSVMfit(S,X,y):

    # Make a countour map of the decision function
    # generate a meshgrid
    numbins = 10

    x1 = np.linspace(X[:,0].min(),X[:,0].max(), numbins)
    x2 = np.linspace(X[:,1].min(),X[:,1].max(), numbins)
    x3 = np.linspace(X[:,2].min(),X[:,2].max(), numbins)
    x4 = np.linspace(X[:,3].min(),X[:,3].max(), numbins)
    x5 = np.linspace(X[:,4].min(),X[:,4].max(), numbins)
    x6 = np.linspace(X[:,5].min(),X[:,5].max(), numbins)
    x7 = np.linspace(X[:,6].min(),X[:,6].max(), numbins)
    x8 = np.linspace(X[:,7].min(),X[:,7].max(), numbins)
    x9 = np.linspace(X[:,8].min(),X[:,8].max(), numbins)
    x10 = np.linspace(X[:,9].min(),X[:,9].max(), numbins)
    x11 = np.linspace(X[:,10].min(),X[:,10].max(), numbins)
    x12 = np.linspace(X[:,11].min(),X[:,11].max(), numbins)
    x13 = np.linspace(X[:,12].min(),X[:,12].max(), numbins)
    x14 = np.linspace(X[:,13].min(),X[:,13].max(), numbins)
    x15 = np.linspace(X[:,14].min(),X[:,14].max(), numbins)
    x16 = np.linspace(X[:,15].min(),X[:,15].max(), numbins)
    x17 = np.linspace(X[:,16].min(),X[:,16].max(), numbins)

    X1,X2,X3, X4,X5, X6, X7,X8,X9,X10,X11,X12,X13,X14,X15,X16,X17 = np.meshgrid(x1, x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17)

    # Now get the decision function into the same format
    XX=np.c_[X1.reshape(-1,1),X2.reshape(-1,1),X3.reshape(-1,1),X4.reshape(-1,1),X5.reshape(-1,1),X6.reshape(-1,1),X7.reshape(-1,1),X8.reshape(-1,1),X9.reshape(-1,1),X10.reshape(-1,1),X11.reshape(-1,1),X12.reshape(-1,1),X13.reshape(-1,1),X14.reshape(-1,1),X15.reshape(-1,1),X16.reshape(-1,1),X17.reshape(-1,1)]
    YY=S.decision_function(XX)
    Y = YY.reshape((numbins,numbins))

    # Determine the levels we want to plot + width of lines
    ymax = np.abs(Y).max()
    lev = np.linspace(-1,1,9)*ymax
    levWidth = np.array([1,1,1,1,3,1,1,1,1])/2

    # Figure out the suppport vectos
    alpha = y*0
    alpha[S.support_]=1 # np.abs(S.dual_coef_)

    sb.scatterplot(X[:,0],X[:,1],hue=y,size=alpha+1,legend=False)
    plt.contour(X1,X2,Y,levels=lev,colors='black',linewidths=levWidth)
    #print(lev)

#X_SVM_2features = np.zeros((129,17))
#X_SVM_2features[:,0] = Xtrain_LASSO.loc[:,'NUM_LESIONS']
#X_SVM_2features[:,1] = Xtrain_LASSO.loc[:,'AGE']
#X_SVM_2features[:,2] = Xtrain_LASSO.loc[:,'SEX']
#X_SVM_2features[:,3] = Xtrain_LASSO.loc[:,'PRIMARY_IN_SITU']
#X_SVM_2features[:,4] = Xtrain_LASSO.loc[:,'DFI']
#X_SVM_2features[:,5] = Xtrain_LASSO.loc[:, 'PREV_SYS_TX']
#X_SVM_2features[:,6] = Xtrain_LASSO.loc[:,'PREV_SYS_ADJUV']
#X_SVM_2features[:,7] = Xtrain_LASSO.loc[:,'PREV_SYS_DEFIN_NUMBER_LINES']
#X_SVM_2features[:,8] = Xtrain_LASSO.loc[:,'PREV_LOCAL_TX']
#X_SVM_2features[:,9] = Xtrain_LASSO.loc[:,'PREV_RFA']
#X_SVM_2features[:,10] = Xtrain_LASSO.loc[:,'NON_SBRT_METS_PREV_SURG']
#X_SVM_2features[:,11] = Xtrain_LASSO.loc[:,'NON_SBRT_METS_PREV_RFA']
##X_SVM_2features[:,12] = Xtrain_LASSO.loc[:,'LOCATION_2.0']
#X_SVM_2features[:,13] = Xtrain_LASSO.loc[:,'LOCATION_4.0']
#X_SVM_2features[:,14] = Xtrain_LASSO.loc[:,'PRIMARY_LOCATION_1.0']
##X_SVM_2features[:,15] = Xtrain_LASSO.loc[:,'PRIMARY_LOCATION_3.0']
#X_SVM_2features[:,16] = Xtrain_LASSO.loc[:,'PRIMARY_LOCATION_5.0']

#SVM model and the confusion matrix and corresponding outcomes.
ytrain_np = np.array(ytrain)
S = svm.SVC(C=100,kernel='rbf',gamma=1,probability=True) #kernel istype of line used to divide, line is how exact the selection is
#less strict = more c
S.fit(Xtrain_LASSO,ytrain)
#plotSVMfit(S,Xtrain_LASSO,ytrain_np)
ypredSVM = S.predict(Xtest_LASSO)
compute_performance(ypredSVM,ytest)
plot_confusion_matrix(ytest,ypredSVM,classes = unique_labels([0,1]))
ypred
ypredSVMProba = S.predict_proba(Xtest_LASSO)

#Random forest classifier and the corresponding calculations.
clf=RandomForestClassifier(n_estimators=100,random_state=0,min_samples_leaf=1,min_samples_split=2,bootstrap=True)
clf = clf.fit(Xtrain_LASSO,ytrain)

ytop=clf.predict(Xtest_LASSO)
ypredCLFProba = clf.predict_proba(Xtest_LASSO)

compute_performance(ytop,ytest)
plot_confusion_matrix(ytest,ytop,classes = unique_labels([0,1]))

#ROC and AUC for the random forest classifier
fprcf,tprcf,thresholds = roc_curve(ytest,ypredCLFProba[:,1])

AUC_value = auc(fprcf,tprcf)

ax = plt.plot(fprcf,tprcf,label=AUC_value)
plt.title('ROC Curve for RFC Classification')
plt.legend()
plt.xlabel('FPR')
plt.ylabel('TPR')
thresholds
compute_performance(ytop, ytest)

#AUC and ROC for SVM.
fprsv,tprsv,thresholds = roc_curve(ytest,ypredSVMProba[:,1])

AUC_value = auc(fprsv,tprsv)

ax = plt.plot(fprsv,tprsv,label=AUC_value)
plt.title('ROC Curve for SVM Classification')
plt.legend()
plt.xlabel('FPR')
plt.ylabel('TPR')
thresholds

compute_performance(ypredSVM,ytest)

#Cross Validate SVM not complete, put on hold
from sklearn.model_selection import ShuffleSplit

#cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)

scoresSVM = cross_val_score(S,Xtest_LASSO,ytest)
scoresRFT = cross_val_score(clf,Xtest_LASSO,ytest)
np.average(scoresRFT)
scoresRFT
